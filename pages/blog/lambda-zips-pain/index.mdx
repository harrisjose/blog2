---
title: Lamdas, zip files and streams
date: 2021-08-20
tags:
  - aws
  - lambda
  - unzip
  - serverless
excerpt: Notes on how stuff can..not work when trying to unzip a zip inside a lambda
draft: false
---

I've been looking into automating our front-end deployments using AWS CodePipeline. We already use S3 as our static file store and CodeBuild to run the builds so CodePipeline was the next step we needed to figure out for CI/CD.

I did not find a way to disable encryption for the artifacts from CodePipeline without messing up some of our existing build setups. So we needed to figure out how to get the zip file from S3, unzip it and put the contents on to the bucket which hosts our static files.

The first version I came up with was using a lambda to unzip the file using streams. Get a read stream for the zip which is then piped to node-unzipper. Each file extracted is then uploaded into the target S3 bucket as they are unzipped.

```javascript:index.js
const AWS = require('aws-sdk')
const unzipper = require('unzipper')

const S3 = new AWS.S3()

exports.handler = async () => {
  const source = //source bucket
  const destination = //destination bucket

  let promises = []

  const readStream = S3.getObject({ Key: zipFileName, Bucket: source }).createReadStream()

  await readStream
    .pipe(unzipper.Parse({ forceStream: true }))


  for await (const entry of zip) {
    const type = entry.type

    if (type === 'File') {
      promises.push(S3.upload({
        Bucket: destination,
        Key: entry.fileName,
        Body: entry,
      }).promise())
    } else {
      entry.autodrain()
    }
  }

  console.log(`Found ${promises.length} files`)

  await Promise.all(promises)
}
```

The good thing about streams is that you dont really load the entire file into memory before you unzip it. So the default 128mb a lambda starts with works for our use case. Downloading the file to disk and unzipping it is an option but I don't really like using the disk from within serverless functions. It just feels dirty.

Huge bummer though, the lambda always stops running after a few files are extracted. I'm not entirely sure if this is a bug within lambda that somehow leads it to think that the [entire event-loop is flushed](https://github.com/ZJONSSON/node-unzipper/issues/189#issuecomment-640144517). Also it turns out zip files aren't really suited to be [unzipped via a stream](https://github.com/thejoshwolfe/yauzl#no-streaming-unzip-api).

The final version I ended up with uses a buffer to store the entire zip file in memory, then uses yauzl to get a readstream of every extracted file. I did have to increase the memory of the lambda to 256mb though but besides that this works as expected.

```javascript:index.js
const yauzl = require('yauzl')
const mime = require('mime-types')
const { promisify } = require('util')

const fromBuffer = promisify(yauzl.fromBuffer)
const S3 = new AWS.S3()

exports.handler = async () => {
  const source = //source bucket
  const destination = //destination bucket

  const { Body: content } = await S3.getObject({ Key: zipFileName, Bucket: source }).promise()

  const zip = await fromBuffer(content)

  return new Promise((resolve, reject) => {
    let promises = []

    zip
      .on('entry', async (entry) => {
        // Directory file names end with '/' so we skip those.
        if (!/\/$/.test(entry.fileName)) {
          const openReadStream = promisify(zip.openReadStream.bind(zip))

          const stream = await openReadStream(entry)
          promises.push(uploadFile(entry, stream))
        }
      })
      .on('end', async () => {
        console.log(`Found ${promises.length} files`)

        await Promise.all(promises)
        resolve()
      })
      .on('error', reject)
  })
}
```
